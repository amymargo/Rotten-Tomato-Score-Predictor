{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amymargo/Rotten-Tomato-Score-Predictor/blob/main/Rotten_Tomato_Score_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1**"
      ],
      "metadata": {
        "id": "jpC5eEdPuIgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Rotten Tomato Movie Scores"
      ],
      "metadata": {
        "id": "YXOX1gnOh_2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-XeP5bEgw7S"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import base64\n",
        "import json\n",
        "\n",
        "list_of_movie_details = {}\n",
        "\n",
        "#function for cleaning the json data that we get from the rotten tomato website\n",
        "#this is a helper function\n",
        "def clean_data_for_rotten_tomatoes(title, audience_score, critics_score, after_cursor):\n",
        "\n",
        "    #for each variable, we check that it is a string instance so there is no data\n",
        "    #integrity problems, then we replace the string types to ensure that if there\n",
        "    #is no problem incase there is a quote in the movie title.\n",
        "\n",
        "    #for each of these we also have a set default so that way we can easily filter them out\n",
        "    #later if there are any problems\n",
        "    if isinstance(title, str):\n",
        "        title = title.replace('\"', \"'\")\n",
        "    else:\n",
        "        title = \"Unknown Title\"\n",
        "\n",
        "    if isinstance(audience_score, str):\n",
        "        audience_score = audience_score\n",
        "    else:\n",
        "        audience_score = \"N/A\"\n",
        "\n",
        "    if isinstance(critics_score, str):\n",
        "        critics_score = critics_score\n",
        "    else:\n",
        "       critics_score =  \"N/A\"\n",
        "    if isinstance(after_cursor, str):\n",
        "        after_cursor = after_cursor\n",
        "    else:\n",
        "        after_cursor = \"Unknown Cursor\"\n",
        "    return title, audience_score, critics_score, after_cursor\n",
        "\n",
        "def fetch_movies(limit=1000):\n",
        "\n",
        "    #this is the base url of the api request\n",
        "    base_url = \"https://www.rottentomatoes.com/napi/browse/movies_at_home/\"\n",
        "\n",
        "    #this was a part of the url that would change depending on the page\n",
        "    #we wanted to go from page to page, and this is what allowed us to do so\n",
        "    after_cursor = base64.b64encode(b\"29\").decode()\n",
        "\n",
        "    #this just indicated that this was not the last page in the list, later will be updated by the response json\n",
        "    has_next_page = True\n",
        "    count = 1\n",
        "\n",
        "\n",
        "    # this is our loop to go on until either there are no more pages or our limit has been reached.\n",
        "    while has_next_page and count < limit:\n",
        "\n",
        "\n",
        "        #here we make the actual request to the API, we used tool that translated\n",
        "        # cUrl request to python, but generally it was pretty straight forward\n",
        "        response = requests.get(\n",
        "            f\"{base_url}?after={after_cursor}\",\n",
        "            headers={\n",
        "                \"accept\": \"*/*\",\n",
        "                \"user-agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Mobile Safari/537.36\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "\n",
        "        #this confirms that the request made was valid\n",
        "        if response.status_code == 200:\n",
        "\n",
        "            #loads the response data, which we will be able to parse\n",
        "            data = response.json()\n",
        "\n",
        "\n",
        "            #there was a lot of unsertainty about ho well this all would work, so we put a lot\n",
        "            #of safegaurds regarding what the data might have (use .get() instead of direct access\n",
        "            # and have the fillers of {} or [] incase they didnt exist)\n",
        "            movies = data.get(\"grid\", {}).get(\"list\", [])\n",
        "            for movie in movies:\n",
        "                title, audience_score, critics_score, after_cursor = clean_data_for_rotten_tomatoes(\n",
        "                    movie.get(\"title\", \"Unknown Title\"),\n",
        "                    movie.get(\"audienceScore\", {}).get(\"scorePercent\", \"N/A\"),\n",
        "                    movie.get(\"criticsScore\", {}).get(\"scorePercent\", \"N/A\"),\n",
        "                    after_cursor\n",
        "                )\n",
        "\n",
        "\n",
        "                #add it to our own dictionary data set\n",
        "                list_of_movie_details[title] = {\n",
        "                    \"audience_score\": audience_score,\n",
        "                    \"critics_score\": critics_score,\n",
        "                    \"after_cursor\": after_cursor\n",
        "                }\n",
        "\n",
        "                # Print progress\n",
        "                print(f\"Movie Count: {count}\")\n",
        "                print(f\"Title: {title}\")\n",
        "                print(f\"Audience Score: {audience_score}\")\n",
        "                print(f\"Critics Score: {critics_score}\")\n",
        "                print(\"-\" * 50)\n",
        "                count += 1\n",
        "\n",
        "\n",
        "            #here we were cacheing out results incase there was a problem with the API\n",
        "            #we wanted to \"save\" our progress as we went along\n",
        "            with open(\"latest_movie_reviews.json\", \"w\") as f:\n",
        "                json.dump(list_of_movie_details, f, indent=4)\n",
        "\n",
        "            #here we just got the page info that contained the has_next_page variable so\n",
        "            #we knew when we reached the end.\n",
        "            page_info = data.get(\"pageInfo\", {})\n",
        "            has_next_page = page_info.get(\"hasNextPage\", False)\n",
        "\n",
        "            #this part we needed to get some help from online sources,\n",
        "            #but we found that the after_cursor simply got us to the next\n",
        "            # page and is fairly standard\n",
        "            after_cursor = page_info.get(\"endCursor\", \"\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Failed to fetch data: {response.status_code}\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD25_-migw7U"
      },
      "outputs": [],
      "source": [
        "#This function scrapes the rotten tomato page and pulls all the names of the movies\n",
        "#the rotten critic score and audiance score\n",
        "#it does this by making a question to the public API url that is used to load the page, and then\n",
        "#read the json.\n",
        "\n",
        "#Since we are working with a large data set it we made it so that the data was constantly dumped into a .json file\n",
        "#incase anything went wrng along the way.\n",
        "fetch_movies(limit=10) #limited to 10 for this example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Youtube Comments from Movie Trailers"
      ],
      "metadata": {
        "id": "uK3eE-SDiDyg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQA61U1kgw7U"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "class Sentiment:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "\n",
        "    #this is used to generate a sentiment analysis\n",
        "    #we put it in a class to make the code cleaner, and\n",
        "    #so that way we can track of errors without distrupting flow\n",
        "    def analyze_sentiment(self, text):\n",
        "        try:\n",
        "            analysis = TextBlob(text)\n",
        "            return analysis.sentiment.polarity\n",
        "        except:\n",
        "            self.count += 1\n",
        "            print(\"problem happened\")\n",
        "            return 0\n",
        "\n",
        "    #returns error count\n",
        "    def countBad(self):\n",
        "        print(self.count)\n",
        "\n",
        "class YouTube():\n",
        "    def __init__(self, APIKEY):\n",
        "\n",
        "        #the following are simply used to activate our YouTube connectionss\n",
        "        self.API_KEY = APIKEY\n",
        "        self.YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "        self.YOUTUBE_API_VERSION = 'v3'\n",
        "        self.youtube = build(self.YOUTUBE_API_SERVICE_NAME, self.YOUTUBE_API_VERSION, developerKey=self.API_KEY)\n",
        "\n",
        "\n",
        "    def clean_comment(self,x):\n",
        "        #returns x if x is none\n",
        "        if x is None:\n",
        "            return x\n",
        "        #cleans the code so that way only characters we want are in there\n",
        "        x = re.sub(r\"[^a-zA-Z0-9\\s.,!?\\'\\\"-]\", '', x)\n",
        "        #finally we also added a check so that there is no HTML code in there which we found to be problematic later\n",
        "        return re.sub(r'<.*?>', '', x)\n",
        "\n",
        "\n",
        "    #this function is used for getting a list of video comments based on a video_id\n",
        "    def get_video_comments(self, video_id, max_results=10):\n",
        "        comments = []\n",
        "        next_page_token = None\n",
        "\n",
        "        while True:\n",
        "\n",
        "            #this builds a request to youtube's commentThreads API  and fetches the top comments\n",
        "            request = self.youtube.commentThreads().list(\n",
        "                part=\"snippet\",\n",
        "                #sets the video ID\n",
        "                videoId=video_id,\n",
        "                # API allows max of 100 per page\n",
        "                maxResults=min(max_results, 100),\n",
        "                #similar to the rotten tomato request, this keeps track if we need to move to next page\n",
        "                pageToken=next_page_token\n",
        "\n",
        "            )\n",
        "\n",
        "            #this makes the request and returns the json data\n",
        "            response = request.execute()\n",
        "\n",
        "            #here we do some basic parsing, the json data is nested so we need to pull only what we need out of it\n",
        "            for item in response['items']:\n",
        "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "                #we then append it to a list of comments and clean it as well.\n",
        "                comments.append(self.clean_comment(comment))\n",
        "\n",
        "            # get next page or break if no more pages\n",
        "            next_page_token = response.get('nextPageToken')\n",
        "            if not next_page_token or len(comments) >= max_results:\n",
        "                break\n",
        "\n",
        "        return comments\n",
        "\n",
        "\n",
        "    def get_channel_id(self, channel_name):\n",
        "\n",
        "        #here we take advantage of youtubes search function\n",
        "        #we pass in the channel name, type and number of results (which is\n",
        "        # just a techincial thing we must include)\n",
        "        request = self.youtube.search().list(\n",
        "            part=\"snippet\",\n",
        "            q=channel_name,\n",
        "            type=\"channel\",\n",
        "            maxResults=1\n",
        "        )\n",
        "        response = request.execute()\n",
        "\n",
        "        #same as before, we parse through the data to extract only what we really need and return an\n",
        "        #error if it is not there, in this case, the channel does not exisit\n",
        "        if 'items' in response and response['items']:\n",
        "            return response['items'][0]['id']['channelId']\n",
        "        else:\n",
        "            raise ValueError(f\"Channel '{channel_name}' not found.\")\n",
        "\n",
        "    def get_video_names_and_ids(self, channel_name, max_results=50):\n",
        "\n",
        "        videos = []\n",
        "        next_page_token = None\n",
        "        #we begin by using one of the other functions to get the channel ID\n",
        "        channel_id = self.get_channel_id(channel_name)\n",
        "\n",
        "        #then run a loop that will continue until all of the videos are retreived\n",
        "        while True:\n",
        "            #here we make the request to the youtube API\n",
        "            request = self.youtube.search().list(\n",
        "                part=\"snippet\",\n",
        "                channelId=channel_id,\n",
        "                maxResults=min(max_results, 50),  # API allows up to 50 per request\n",
        "                pageToken=next_page_token,\n",
        "                type=\"video\"\n",
        "            )\n",
        "            response = request.execute()\n",
        "\n",
        "            #sort through the items to get only what we need, then append it to the list\n",
        "            for item in response.get('items', []):\n",
        "                video_id = item['id'].get('videoId')\n",
        "                video_title = item['snippet']['title']\n",
        "                if video_id:\n",
        "                    videos.append((video_id, video_title))\n",
        "\n",
        "            next_page_token = response.get('nextPageToken')\n",
        "            if not next_page_token or len(videos) >= max_results:\n",
        "                break\n",
        "\n",
        "        return videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e993fbgUgw7U"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "#Here we initilized the youtube class so we can interact with the API\n",
        "youtubeAdmin = YouTube(APIKEY)\n",
        "\n",
        "#Here we selected which youtube channels we wanted to grab the trailers from\n",
        "list_of_channels = [\"WarnerBrosPictures\", \"20thCenturyStudios\", \"marvel\", \"AppleTV\", \"A24\", \"Showtime\",\"Netflix\"]\n",
        "\n",
        "\n",
        "#Here we filter through all videos that are movie trailers and store their title and youtubeID\n",
        "#by analyzing the youtube video title format we saw that most trailers included the word \"trailer\"\n",
        "#so we filtered for that, while we didnt catch ALL trailers, this was an easy quick filter\n",
        "\n",
        "#We also grabbed the movie name so that way we can do manual checks later on\n",
        "list_of_movies = []\n",
        "for channel_name in list_of_channels:\n",
        "    movies_in_channel = youtubeAdmin.get_video_names_and_ids(channel_name)\n",
        "    for id, title in movies_in_channel:\n",
        "        if \"trailer\" in title.lower():\n",
        "            # print(title)\n",
        "            list_of_movies.append((id, title.lower()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7paU2LKOgw7V"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "#open up the rotten tomato data and load as python json object\n",
        "#Remember that this was the data that we grabbed from before\n",
        "with open(\"latest_movie_reviews.json\", \"r\") as f:\n",
        "    rt_scores = json.loads(f.read())\n",
        "\n",
        "#find matches between the two data sets\n",
        "#we generated the list of ignore_set by looking at the data that we had and found that there were movie title that were\n",
        "#fairly generic and would be better off if we ignored them as to not cause any problems\n",
        "#since our youtube trailer set included aditional words aside from just the movie name\n",
        "ignore_set = ['Life','Gold','Heat', 'Go', 'Us', 'Pi', 'Steel', 'Ali', 'Plane', 'Man of Steel', 'App', 'Logan', 'Room', 'The Killing', 'War', 'IF', 'Moon', 'Heretic', 'The Zone of Interest', 'Ma', 'Red', 'RV', 'It', 'AfrAId', 'Ted', 'The Many Saints of Newark', 'Dick', 'Bird', 'Ran', 'Up', 'The Killing of a Sacred Deer', 'Beau Is Afraid', 'Old', 'Kill', 'Black Widow', 'IO', 'Bohemian Rhapsody', 'Her', 'Murder on the Orient Express', 'Civil War', 'After']\n",
        "list_of_matches = []\n",
        "title_count = {}\n",
        "\n",
        "#we then itterated through all of the rt scores\n",
        "for rt_movie_title in rt_scores:\n",
        "\n",
        "    #checked if the title was valid, and that it is not in the ignore set\n",
        "    if len(rt_movie_title) > 1 and rt_movie_title not in ignore_set:\n",
        "\n",
        "        #we then itterated through our youtube movie trailer list\n",
        "        #replaced some filler words that we noticed would cause problems\n",
        "        #then we would standardize the string and check if we have a match\n",
        "        #if there was a mach we would append it to a list for use later on\n",
        "        for movie in list_of_movies:\n",
        "            movie_title = movie[1]\n",
        "            movie_title = movie_title.replace(\"marvel studios\", \"\")\n",
        "            movie_title = movie_title.replace(\"final trailer\", \"\")\n",
        "\n",
        "            if rt_movie_title.lower() in movie_title:\n",
        "                list_of_matches.append((movie[0], rt_movie_title, rt_scores[rt_movie_title]))\n",
        "\n",
        "\n",
        "#at this point we noticed that there was still inconsitancy in the data\n",
        "#since the data set contained ~200 movies by this point, it was failry easy to\n",
        "#manually do some cleanying and confirm that the movies we had matched up\n",
        "#were truely the correct matches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create CSV"
      ],
      "metadata": {
        "id": "uctLhkCQhyXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HpWs1_Mgw7V"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "#for simple storage, here we have our list of good matches.\n",
        "good_matches = [('iZwykQK9aZo', 'Blitz', {'audience_score': '36%', 'critics_score': '48%', 'after_cursor': 'MjA2OQ=='}), ('_9CmC5Rmsdw', 'A Different Man', {'audience_score': '80%', 'critics_score': '92%', 'after_cursor': 'NTk='}), ('mqqft2x_Aa4', 'The Batman', {'audience_score': '87%', 'critics_score': '85%', 'after_cursor': 'ODk='}), ('kymDzCgPwj0', 'I Saw the TV Glow', {'audience_score': '71%', 'critics_score': '84%', 'after_cursor': 'ODk='}), ('D30r0CwtIKc', 'The Whale', {'audience_score': '84%', 'critics_score': '86%', 'after_cursor': 'NzE2OQ=='}), ('sw7RElt-SvE', 'Paddington 2', {'audience_score': '88%', 'critics_score': '99%', 'after_cursor': 'MTE5'}), ('1HZAnkxdYuA', 'The Little Things', {'audience_score': '67%', 'critics_score': '44%', 'after_cursor': 'MTE5'}), ('G9jOaggGPKQ', 'Aftersun', {'audience_score': '81%', 'critics_score': '95%', 'after_cursor': 'MTQ5'}), ('EG0si5bSd6I', 'Killers of the Flower Moon', {'audience_score': '84%', 'critics_score': '93%', 'after_cursor': 'MTc5'}), ('X5oqpxi3U7M', 'The Instigators', {'audience_score': '54%', 'critics_score': '40%', 'after_cursor': 'MjA5'}), ('aLAKJu9aJys', 'Talk to Me', {'audience_score': '82%', 'critics_score': '94%', 'after_cursor': 'MjM5'}), ('5wfrDhgUMGI', 'Hidden Figures', {'audience_score': '93%', 'critics_score': '93%', 'after_cursor': 'MjY5'}), ('V6wWKNij_1M', 'Hereditary', {'audience_score': '71%', 'critics_score': '90%', 'after_cursor': 'Mjk5'}), ('x_me3xsvDgk', 'Eternals', {'audience_score': '77%', 'critics_score': '47%', 'after_cursor': 'Mjk5'}), ('cNi_HC839Wo', 'Lady Bird', {'audience_score': '79%', 'critics_score': '99%', 'after_cursor': 'Mzg5'}), ('LdOM0x0XDMo', 'Tenet', {'audience_score': '76%', 'critics_score': '70%', 'after_cursor': 'Mzg5'}), ('9ix7TUGVYIo', 'The Matrix Resurrections', {'audience_score': '63%', 'critics_score': '63%', 'after_cursor': 'NDQ5'}), ('aWzlQ2N6qqg', 'Doctor Strange in the Multiverse of Madness', {'audience_score': '85%', 'critics_score': '74%', 'after_cursor': 'NDQ5'}), ('8KVsaoveTbw', 'The Iron Claw', {'audience_score': '94%', 'critics_score': '89%', 'after_cursor': 'NDc5'}), ('43NWzay3W4s', 'Captain America: Civil War', {'audience_score': '89%', 'critics_score': '90%', 'after_cursor': 'NjI5'}), ('qvqyBWCN39o', 'Tuesday', {'audience_score': '50%', 'critics_score': '76%', 'after_cursor': 'NjU5'}), ('20GWk5cWPBs', 'You Hurt My Feelings', {'audience_score': '64%', 'critics_score': '94%', 'after_cursor': 'NjU5'}), ('U0CL-ZSuCrQ', 'In the Heights', {'audience_score': '94%', 'critics_score': '94%', 'after_cursor': 'NzE5'}), ('fKHVHzCGmF0', 'Sharper', {'audience_score': '73%', 'critics_score': '68%', 'after_cursor': 'ODM5'}), ('0pmfrE1YL4I', 'CODA', {'audience_score': '91%', 'critics_score': '94%', 'after_cursor': 'ODY5'}), ('zyYgDtY2AMY', 'Ford v Ferrari', {'audience_score': '98%', 'critics_score': '92%', 'after_cursor': 'ODY5'}), ('R_F-lVhSfx8', 'Black Mass', {'audience_score': '68%', 'critics_score': '73%', 'after_cursor': 'OTU5'}), ('ByehYal_cCs', 'Cloud Atlas', {'audience_score': '66%', 'critics_score': '66%', 'after_cursor': 'MTEwOQ=='}), ('nSbzyEJ8X9E', 'A Star Is Born', {'audience_score': '81%', 'critics_score': '98%', 'after_cursor': 'ODYwOQ=='}), ('eMQt_V2eCRY', 'The Front Room', {'audience_score': '42%', 'critics_score': '42%', 'after_cursor': 'MTE2OQ=='}), ('A5GJLwWiYSg', 'West Side Story', {'audience_score': '84%', 'critics_score': '92%', 'after_cursor': 'NjQxOQ=='}), ('DBWk6BohVXk', 'Priscilla', {'audience_score': '63%', 'critics_score': '84%', 'after_cursor': 'MTIyOQ=='}), ('5sEaYB4rLFQ', 'Fantastic Beasts: The Crimes of Grindelwald', {'audience_score': '53%', 'critics_score': '36%', 'after_cursor': 'MTI1OQ=='}), ('GLs2xxM0e78', 'Amsterdam', {'audience_score': '62%', 'critics_score': '32%', 'after_cursor': 'MTI4OQ=='}), ('LTNZmOJxuAc', 'The Lobster', {'audience_score': '65%', 'critics_score': '88%', 'after_cursor': 'MTMxOQ=='}), ('ZSzeFFsKEt4', 'The Hobbit: The Battle of the Five Armies', {'audience_score': '74%', 'critics_score': '59%', 'after_cursor': 'MTM0OQ=='}), ('ptqe7s6pO7g', 'The Tragedy of Macbeth', {'audience_score': '74%', 'critics_score': '92%', 'after_cursor': 'MTM3OQ=='}), ('6Nxc-3WpMbg', 'Kingsman: The Golden Circle', {'audience_score': '64%', 'critics_score': '51%', 'after_cursor': 'MTQwOQ=='}), ('gSMxBLlA8qY', 'Richard Jewell', {'audience_score': '96%', 'critics_score': '77%', 'after_cursor': 'MTQzOQ=='}), ('Dp2ufFO4QGg', 'The Art of Racing in the Rain', {'audience_score': '96%', 'critics_score': '44%', 'after_cursor': 'MTQzOQ=='}), ('N_yfmHCkSB0', 'Boston Strangler', {'audience_score': '68%', 'critics_score': '68%', 'after_cursor': 'MTU1OQ=='}), ('0dCfbBwFI2Y', 'The Spectacular Now', {'audience_score': '76%', 'critics_score': '92%', 'after_cursor': 'MTU4OQ=='}), ('n0UFgbGZk10', 'Problemista', {'audience_score': '85%', 'critics_score': '86%', 'after_cursor': 'MTYxOQ=='}), ('nvP-ZrzA2lk', 'Lincoln', {'audience_score': '80%', 'critics_score': '90%', 'after_cursor': 'MTc2OQ=='}), ('j307q8zMD3Y', 'Causeway', {'audience_score': '72%', 'critics_score': '85%', 'after_cursor': 'MTgyOQ=='}), ('GVQbeG5yW78', 'Just Mercy', {'audience_score': '99%', 'critics_score': '85%', 'after_cursor': 'MTg1OQ=='}), ('kjC1zmZo30U', 'Tag', {'audience_score': '57%', 'critics_score': '55%', 'after_cursor': 'MTg1OQ=='}), ('Kwp32zLc08c', 'After Yang', {'audience_score': '68%', 'critics_score': '89%', 'after_cursor': 'MTk3OQ=='}), ('ZBvK6ni97W8', 'The Judge', {'audience_score': '72%', 'critics_score': '49%', 'after_cursor': 'MjMwOQ=='}), ('tN8o_E_f9FQ', 'The Darkest Minds', {'audience_score': '71%', 'critics_score': '15%', 'after_cursor': 'MjM5OQ=='}), ('sSjtGqRXQ9Y', 'Judas and the Black Messiah', {'audience_score': '95%', 'critics_score': '96%', 'after_cursor': 'MjgxOQ=='}), ('mjKEXxO2KNE', 'Sully', {'audience_score': '84%', 'critics_score': '85%', 'after_cursor': 'MjgxOQ=='}), ('w9Rx6-GaSIE', 'Mid90s', {'audience_score': '81%', 'critics_score': '81%', 'after_cursor': 'MjkzOQ=='}), ('fH0cEP0mvlU', 'This Is Where I Leave You', {'audience_score': '60%', 'critics_score': '44%', 'after_cursor': 'Mjk5OQ=='}), ('3MM8OkVT0hw', 'The Hate U Give', {'audience_score': '81%', 'critics_score': '97%', 'after_cursor': 'MzM1OQ=='}), ('m5LmfARzwDU', 'Shallow Hal', {'audience_score': '45%', 'critics_score': '49%', 'after_cursor': 'MzU2OQ=='}), ('-rjMwSTeVeo', 'Stop Making Sense', {'audience_score': '97%', 'critics_score': '100%', 'after_cursor': 'MzgwOQ=='}), ('1O3iRdiplB0', 'The Heat', {'audience_score': '71%', 'critics_score': '65%', 'after_cursor': 'MzkyOQ=='}), ('IrabKK9Bhds', 'They Shall Not Grow Old', {'audience_score': '91%', 'critics_score': '99%', 'after_cursor': 'Mzk1OQ=='}), ('DmmHvnS0IKM', 'Blinded by the Light', {'audience_score': '91%', 'critics_score': '89%', 'after_cursor': 'NDA0OQ=='}), ('Af77C4zUkjs', 'Come From Away', {'audience_score': '95%', 'critics_score': '98%', 'after_cursor': 'NDEwOQ=='}), ('qOYrpk-A45s', 'Occupied City', {'audience_score': '75%', 'critics_score': '72%', 'after_cursor': 'NDEwOQ=='}), ('UIMYNVkZBSo', 'Hall Pass', {'audience_score': '40%', 'critics_score': '33%', 'after_cursor': 'NDE5OQ=='}), ('OOPHFQZ5aiM', 'Water for Elephants', {'audience_score': '70%', 'critics_score': '60%', 'after_cursor': 'NDI1OQ=='}), ('VpUeQV8sdOc', 'Horrible Bosses', {'audience_score': '70%', 'critics_score': '69%', 'after_cursor': 'NDI1OQ=='}), ('6JnFaltqnAY', '20th Century Women', {'audience_score': '74%', 'critics_score': '88%', 'after_cursor': 'NDYxOQ=='}), ('NcE2fj-EtJY', 'The Bling Ring', {'audience_score': '33%', 'critics_score': '60%', 'after_cursor': 'NDY3OQ=='}), ('hyzQjVUmIxk', 'Eddie the Eagle', {'audience_score': '82%', 'critics_score': '82%', 'after_cursor': 'NTQyOQ=='}), ('lD41XdWcmbY', 'Shaft', {'audience_score': '68%', 'critics_score': '88%', 'after_cursor': 'ODYzOQ=='}), ('6EJGnU2AmV4', 'Close', {'audience_score': '35%', 'critics_score': '35%', 'after_cursor': 'NTY2OQ=='}), ('KBDE4uznmIw', 'Bride Wars', {'audience_score': '51%', 'critics_score': '10%', 'after_cursor': 'NTkzOQ=='}), ('go1jaIRQc-o', 'Breakthrough', {'audience_score': '81%', 'critics_score': '60%', 'after_cursor': 'NTk2OQ=='}), ('5hJR8hEsLZU', 'The Eternal Daughter', {'audience_score': '46%', 'critics_score': '95%', 'after_cursor': 'NjA4OQ=='}), ('JjysgllBzHQ', 'Life After Beth', {'audience_score': '31%', 'critics_score': '46%', 'after_cursor': 'NjIwOQ=='}), ('i5l6a5RiR1E', 'Stuber', {'audience_score': '79%', 'critics_score': '43%', 'after_cursor': 'NjIwOQ=='}), ('IZUrhfCl0Xc', 'Bad Sister', {'audience_score': '56%', 'critics_score': '', 'after_cursor': 'NjYyOQ=='}), ('dGBe8xsWGlo', 'The Death of Dick Long', {'audience_score': '86%', 'critics_score': '75%', 'after_cursor': 'NzE5OQ=='}), ('6tC1yOUvvMo', 'Jersey Boys', {'audience_score': '62%', 'critics_score': '51%', 'after_cursor': 'NzQwOQ=='}), ('nPfYXXg65qA', 'Keeping Up With the Joneses', {'audience_score': '37%', 'critics_score': '20%', 'after_cursor': 'Nzc2OQ=='}), ('VP5FW5KA6Go', 'Ramona and Beezus', {'audience_score': '71%', 'critics_score': '71%', 'after_cursor': 'Nzc5OQ=='}), ('ld4eE2HU-ig', 'The Exception', {'audience_score': '67%', 'critics_score': '75%', 'after_cursor': 'ODE4OQ=='}), ('6pwnwnzk8L8', 'Chasing Mavericks', {'audience_score': '70%', 'critics_score': '32%', 'after_cursor': 'ODc4OQ=='})]\n",
        "\n",
        "#once again start an instance of our youtube class but now also an instance of the sentimet class for analysis.\n",
        "youtubeAdmin = YouTube(APIKEY)\n",
        "sentimetAdmin = Sentiment()\n",
        "\n",
        "\n",
        "#here things get a little funky, when trying to complete this part using standard pandas approach,\n",
        "#I found that the data would get mixed up and disorded. This is likely due to different languages, emojies,\n",
        "#and odd characters, however we were unable to easily resolve it.\n",
        "\n",
        "#instead since I had some experiance working with the csv library, I decided to write a script that would generate\n",
        "#the csv directly.\n",
        "\n",
        "#here we open the file,\n",
        "with open('movies_comments_cleanTest_with_sentimate.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "\n",
        "    #initialize our write\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    #define what our column headers would be, here are the core 4, however we will have many more generated below\n",
        "    headers = ['trailer_id', 'movie_name', 'audience_score', 'critics_score']\n",
        "\n",
        "    #this will generate our comment columns (1-100)\n",
        "    for i in range(1, 101):\n",
        "        headers.append(f'comment_{i}')\n",
        "\n",
        "    #this will gen our sentiment (1-100)\n",
        "    for i in range(1, 101):\n",
        "        headers.append(f'comment_sentiment_{i}')\n",
        "\n",
        "    #this will write out column row\n",
        "    writer.writerow(headers)\n",
        "\n",
        "    #counter here is just used to keep track of progress\n",
        "    count = 0\n",
        "\n",
        "    #here we through every match we have\n",
        "    #get the youtube comments from the API\n",
        "    #pass them through the sentiment function\n",
        "    #then write all this data into the corrilating columns\n",
        "    for i in good_matches:\n",
        "\n",
        "        count += 1\n",
        "        trailer_id = i[0]\n",
        "        movie_name = i[1]\n",
        "        audience_score = i[2][\"audience_score\"]\n",
        "        critics_score = i[2][\"critics_score\"]\n",
        "        try:\n",
        "\n",
        "            comments = youtubeAdmin.get_video_comments(trailer_id, max_results=100)\n",
        "\n",
        "        except:\n",
        "            print(f\"Youtube Problem Comments for {movie_name}: {len(comments)}\")\n",
        "            comments = []\n",
        "\n",
        "        try:\n",
        "            comments_sentimate = [sentimetAdmin.analyze_sentiment(x) for x in comments]\n",
        "        except:\n",
        "            print(f\"Sentimate for {movie_name}: {len(comments_sentimate)}\")\n",
        "            comments_sentimate = []\n",
        "        # Create row\n",
        "        row = [trailer_id, movie_name, audience_score, critics_score] + comments + comments_sentimate\n",
        "        writer.writerow(row)\n",
        "        print(f\"Success count: {count}\")\n",
        "\n",
        "#at this point the data is all ready for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MwuSqWngw7W"
      },
      "outputs": [],
      "source": [
        "#check that all worked fine\n",
        "sentimetAdmin.countBad()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2**"
      ],
      "metadata": {
        "id": "5INFMH3_uM-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load, Validate, and Split the Data"
      ],
      "metadata": {
        "id": "zqfuNiw8iil9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv('movies_comments_cleanTest_with_sentimate.csv', encoding='utf-8', on_bad_lines='skip',  engine='python')\n",
        "\n",
        "# Define valid columns\n",
        "valid_columns = [f'comment_sentiment_{i}' for i in range(1, 101)]\n",
        "\n",
        "# Drop invalid rows(rows that dont have values in all the comment sentiment columns)\n",
        "df_valid = df.dropna(subset=valid_columns).copy()\n",
        "\n",
        "# Drop rows with over 43 columns of sentiment equal to 0\n",
        "df_valid[\"zero_count\"] = (df_valid[valid_columns] == 0).sum(axis=1)\n",
        "df_valid_reduced = df_valid[df_valid[\"zero_count\"] <= 43].drop(\"zero_count\", axis=1)\n",
        "\n",
        "# Ensure sentiment scores are numerical, if not they will become NaN\n",
        "for col in valid_columns:\n",
        "    df_valid_reduced[col] = pd.to_numeric(df_valid_reduced[col])\n",
        "\n",
        "# Assign the sentiment columns to X, filling all NaN with 0\n",
        "X = df_valid_reduced[valid_columns].fillna(0)\n",
        "\n",
        "# Assign critics_score to y (remove '%' and convert to float)\n",
        "y = pd.to_numeric(df_valid_reduced['critics_score'].str.rstrip('%'))\n",
        "\n",
        "# Drop rows where y is NaN\n",
        "X = X[~y.isna()]\n",
        "y = y[~y.isna()]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of y: {y.shape}\")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Testing set size: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "_uAfQQkdijBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Regression Model"
      ],
      "metadata": {
        "id": "Y2ru8SM0ikru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# Computes weights inversely proportional to the frequency of scores to balance skewed distribution\n",
        "def compute_weights(critic_scores):\n",
        "    bins = [0, 20, 40, 60, 80, 100]\n",
        "    counts, _ = np.histogram(critic_scores, bins=bins)\n",
        "    weights = 1 / np.sqrt(counts + 1e-6)\n",
        "    bin_indices = np.digitize(critic_scores, bins) - 1\n",
        "    bin_indices = np.clip(bin_indices, 0, len(weights) - 1)\n",
        "    return weights[bin_indices]\n",
        "\n",
        "# Initialize model with regularization\n",
        "model = xgb.XGBRegressor(\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=200,\n",
        "    max_depth=4,\n",
        "    subsample=0.8,          # Use 80% of the data per tree\n",
        "    colsample_bytree=0.8,   # Use 80% of the features per tree\n",
        "    reg_alpha=0.1,          # L1 regularization term\n",
        "    reg_lambda=1.0,         # L2 regularization term\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "#Train model with weights\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    sample_weight=compute_weights(y_train)\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate percent accuracy within 10% in test set\n",
        "percentage_difference_test = 100 * np.abs(y_pred - y_test) / y_test\n",
        "accurate_within_10_percent_test = np.sum(percentage_difference_test <= 10)\n",
        "accuracy_percentage_test = 100 * accurate_within_10_percent_test / len(y_test)\n",
        "# Calculate percent accuracy within 5% in test set\n",
        "accurate_within_5_percent_test = np.sum(percentage_difference_test <= 5)\n",
        "accuracy_percentage_5_test = 100 * accurate_within_5_percent_test / len(y_test)\n",
        "\n",
        "# Calculate percent accuracy within 10% in full dataset\n",
        "y_pred_all = model.predict(X)\n",
        "percentage_difference_test_all = 100 * np.abs(y_pred_all - y) / y\n",
        "accurate_within_10_percent_test_all = np.sum(percentage_difference_test_all <= 10)\n",
        "accuracy_percentage_test_all = 100 * accurate_within_10_percent_test_all / len(y)\n",
        "# Calculate percent accuracy within 5% in full dataset\n",
        "accurate_within_5_percent_test_all = np.sum(percentage_difference_test_all <= 5)\n",
        "accuracy_percentage_5_test_all = 100 * accurate_within_5_percent_test_all / len(y)\n",
        "\n",
        "# Print results\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")\n",
        "print(f\"RÂ²: {r2}\\n\")\n",
        "print(f\"Percent accuracy within 10% in test set: {accuracy_percentage_test:.2f}%\")\n",
        "print(f\"Percent accuracy within 10% in full dataset: {accuracy_percentage_test_all:.2f}%\\n\")\n",
        "print(f\"Percent accuracy within 5% in test set: {accuracy_percentage_5_test:.2f}%\")\n",
        "print(f\"Percent accuracy within 5% in full dataset: {accuracy_percentage_5_test_all:.2f}%\")"
      ],
      "metadata": {
        "id": "IPhXhiAFioET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store Predicted Scores in an SQL Database"
      ],
      "metadata": {
        "id": "3cgwSYr6ipyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('movies_scores.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table if it doesn't exist\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS PredictedScores (\n",
        "    MovieID INTEGER,\n",
        "    Title TEXT,\n",
        "    AudienceScore REAL,\n",
        "    CriticScore REAL,\n",
        "    PredictedScore REAL\n",
        ")\n",
        "''')\n",
        "\n",
        "# Clear the table if it exists\n",
        "cursor.execute('DELETE FROM PredictedScores')\n",
        "\n",
        "for i, row in df_valid.iterrows():\n",
        "    try:\n",
        "        # Predict scores for each row\n",
        "        features = pd.DataFrame([row.loc[X.columns].values], columns=X.columns)\n",
        "        predicted_score = float(model.predict(features)[0])\n",
        "\n",
        "        # Insert into the SQL table\n",
        "        cursor.execute('''\n",
        "        INSERT INTO PredictedScores (MovieID, Title, AudienceScore, CriticScore, PredictedScore)\n",
        "        VALUES (?, ?, ?, ?, ?)\n",
        "        ''', (row['trailer_id'], row['movie_name'], row['audience_score'], row['critics_score'], predicted_score))\n",
        "    except KeyError as e:\n",
        "        print(f\"Skipping row {i} due to missing key: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping row {i} due to error: {e}\")\n",
        "\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "BV7OkB7Zirjv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the table\n",
        "query = 'SELECT AudienceScore, CriticScore, PredictedScore FROM PredictedScores'\n",
        "df = pd.read_sql_query(query, conn)\n",
        "print(df)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "jaCF5rFYivoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Data and Model Performance"
      ],
      "metadata": {
        "id": "zRHdOP4WiwGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Connect to db\n",
        "conn = sqlite3.connect('movies_scores.db')\n",
        "query = 'SELECT * FROM PredictedScores'\n",
        "df = pd.read_sql_query(query, conn)\n",
        "\n",
        "# Ensure data is numeric\n",
        "df['AudienceScore'] = pd.to_numeric(df['AudienceScore'].str.rstrip('%'))\n",
        "df['CriticScore'] = pd.to_numeric(df['CriticScore'].str.rstrip('%'))\n",
        "df['PredictedScore'] = pd.to_numeric(df['PredictedScore'])\n",
        "\n",
        "# Visualization 1 - Scatter Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['CriticScore'], df['PredictedScore'], label='Critics vs. Model', alpha=0.7, color='blue')\n",
        "plt.scatter(df['AudienceScore'], df['PredictedScore'], label='Audience vs. Model', alpha=0.7, color='green')\n",
        "plt.plot([30, 100], [30, 100], 'r--', label='Perfect Prediction', alpha=0.8)\n",
        "plt.title(\"Model Predictions vs Critics' and Audience Scores\")\n",
        "plt.xlabel(\"Actual Scores (%)\")\n",
        "plt.ylabel(\"Model Predictions (%)\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2 - Histogram of Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "critics_residuals = df['PredictedScore'] - df['CriticScore']\n",
        "audience_residuals = df['PredictedScore'] - df['AudienceScore']\n",
        "plt.hist(critics_residuals, bins=20, alpha=0.7, label='Critics Residuals', color='blue')\n",
        "plt.hist(audience_residuals, bins=20, alpha=0.7, label='Audience Residuals', color='green')\n",
        "plt.axvline(0, color='red', linestyle='dashed', linewidth=1, label='Zero Residual')\n",
        "plt.title(\"Residuals of Model Predictions\")\n",
        "plt.xlabel(\"Residual Value (Model - Actual)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 3 - Boxplot\n",
        "plt.figure(figsize=(10, 6))\n",
        "boxplot_data = [df['CriticScore'], df['AudienceScore'], df['PredictedScore']]\n",
        "plt.boxplot(boxplot_data, vert=False, patch_artist=True, labels=['Critics', 'Audience', 'Model'])\n",
        "plt.title(\"Distribution of Critics, Audience, and Model Scores\")\n",
        "plt.xlabel(\"Score (%)\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 4 - Score Bar Chart\n",
        "plt.figure(figsize=(15, 8))\n",
        "bar_width = 0.25\n",
        "indices = range(len(df))\n",
        "plt.bar([i - bar_width for i in indices], df['CriticScore'], width=bar_width, label='Critics', color='blue')\n",
        "plt.bar(indices, df['PredictedScore'], width=bar_width, label='Predicted', color='orange')\n",
        "plt.bar([i + bar_width for i in indices], df['AudienceScore'], width=bar_width, label='Audience', color='green')\n",
        "plt.title(\"Critics, Predicted, and Audience Scores for Each Movie\", fontsize=16)\n",
        "plt.xlabel(\"Movies\", fontsize=14)\n",
        "plt.ylabel(\"Scores (%)\", fontsize=14)\n",
        "plt.xticks(indices, df['Title'], rotation=45, ha='right')  # Movie titles on x-axis\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 5 - Error Bar Chart\n",
        "plt.figure(figsize=(15, 8))\n",
        "audience_error = abs(df['PredictedScore'] - df['AudienceScore'])\n",
        "critic_error = abs(df['PredictedScore'] - df['CriticScore'])\n",
        "bar_width = 0.35\n",
        "indices = range(len(df))\n",
        "plt.bar([i - bar_width/2 for i in indices], critic_error, width=bar_width, label='Critics Error', color='blue')\n",
        "plt.bar([i + bar_width/2 for i in indices], audience_error, width=bar_width, label='Audience Error', color='green')\n",
        "plt.title(\"Absolute Errors of Predicted Scores\", fontsize=16)\n",
        "plt.xlabel(\"Movies\", fontsize=14)\n",
        "plt.ylabel(\"Absolute Error (%)\", fontsize=14)\n",
        "plt.xticks(indices, df['Title'], rotation=45, ha='right')  # Movie titles on x-axis\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JTcRxcZEiydj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}